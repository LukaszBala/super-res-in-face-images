{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "xCDflBj9KIh3",
    "ExecuteTime": {
     "end_time": "2024-09-10T13:39:52.362036Z",
     "start_time": "2024-09-10T13:39:52.328880Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import csv\n",
    "import os\n",
    "from RLFN.rlfn import RLFN_S\n",
    "from utils import tensor2uint, tensor_to_uint8, uint2tensor4\n",
    "from choose_device import choose_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T13:39:52.366240Z",
     "start_time": "2024-09-10T13:39:52.352439Z"
    }
   },
   "outputs": [],
   "source": [
    "# %env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T13:39:52.367523Z",
     "start_time": "2024-09-10T13:39:52.361194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'PATH': '/Users/lukaszbala/projects/super-res-in-face-images/venv/bin:/Users/lukaszbala/miniforge3/bin:/Users/lukaszbala/miniforge3/condabin:/Users/lukaszbala/.nvm/versions/node/v19.7.0/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Applications/VMware Fusion.app/Contents/Public:/Users/lukaszbala/Library/Application Support/JetBrains/Toolbox/scripts',\n 'GSETTINGS_SCHEMA_DIR_CONDA_BACKUP': '',\n 'XML_CATALOG_FILES': 'file:///Users/lukaszbala/miniforge3/etc/xml/catalog file:///etc/xml/catalog',\n 'CONDA_DEFAULT_ENV': 'base',\n 'CONDA_EXE': '/Users/lukaszbala/miniforge3/bin/conda',\n 'CONDA_PYTHON_EXE': '/Users/lukaszbala/miniforge3/bin/python',\n 'HOMEBREW_PREFIX': '/opt/homebrew',\n 'LANG': 'en_US.UTF-8',\n 'COMMAND_MODE': 'unix2003',\n 'PS1': '(venv) ',\n 'CONDA_PREFIX': '/Users/lukaszbala/miniforge3',\n 'NVM_INC': '/Users/lukaszbala/.nvm/versions/node/v19.7.0/include/node',\n 'PYTORCH_ENABLE_MPS_FALLBACK': '1',\n '_CE_M': '',\n 'LOGNAME': 'lukaszbala',\n 'HOMEBREW_REPOSITORY': '/opt/homebrew',\n 'XPC_SERVICE_NAME': 'application.com.jetbrains.pycharm.5608025.5608685',\n 'PWD': '/Users/lukaszbala/projects/super-res-in-face-images',\n 'INFOPATH': '/opt/homebrew/share/info:',\n 'CONDA_SHLVL': '1',\n '__CFBundleIdentifier': 'com.jetbrains.pycharm',\n 'LANGUAGE': '',\n 'PYTHONPATH': '/Users/lukaszbala/projects/super-res-in-face-images',\n 'NVM_CD_FLAGS': '-q',\n 'SHELL': '/bin/zsh',\n 'NVM_DIR': '/Users/lukaszbala/.nvm',\n 'OLDPWD': '/',\n 'HOMEBREW_CELLAR': '/opt/homebrew/Cellar',\n 'USER': 'lukaszbala',\n 'TMPDIR': '/var/folders/t3/73s0z9910mj5djg3_8_2lwym0000gn/T/',\n 'SSH_AUTH_SOCK': '/private/tmp/com.apple.launchd.Q3wSpdRV9o/Listeners',\n '_CE_CONDA': '',\n 'VIRTUAL_ENV': '/Users/lukaszbala/projects/super-res-in-face-images/venv',\n 'XPC_FLAGS': '0x0',\n 'LC_ALL': 'en_US.UTF-8',\n '__CF_USER_TEXT_ENCODING': '0x1F5:0x1D:0x2A',\n 'CONDA_PROMPT_MODIFIER': '(base) ',\n 'GSETTINGS_SCHEMA_DIR': '/Users/lukaszbala/miniforge3/share/glib-2.0/schemas',\n 'LC_CTYPE': 'pl_PL.UTF-8',\n 'NVM_BIN': '/Users/lukaszbala/.nvm/versions/node/v19.7.0/bin',\n 'HOME': '/Users/lukaszbala',\n 'JPY_SESSION_NAME': 'master_thesis.ipynb',\n 'JPY_PARENT_PID': '25697',\n 'PYDEVD_USE_FRAME_EVAL': 'NO',\n 'TERM': 'xterm-color',\n 'CLICOLOR': '1',\n 'FORCE_COLOR': '1',\n 'CLICOLOR_FORCE': '1',\n 'PAGER': 'cat',\n 'GIT_PAGER': 'cat',\n 'MPLBACKEND': 'module://matplotlib_inline.backend_inline',\n 'LD_LIBRARY_PATH': '/Users/lukaszbala/projects/super-res-in-face-images/venv/lib/python3.9/site-packages/cv2/../../lib:'}"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_kUE0NYYFI2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def save_losses_to_csv(training_losses, validation_losses, filename):\n",
    "    \"\"\"\n",
    "    Save training and validation losses to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        training_losses (list of float): List of training losses for each epoch.\n",
    "        validation_losses (list of float): List of validation losses for each epoch.\n",
    "        filename (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    # Ensure both lists are of the same length\n",
    "    assert len(training_losses) == len(validation_losses), \"Training and validation losses must have the same length.\"\n",
    "\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Epoch\", \"Training Loss\", \"Validation Loss\"])\n",
    "\n",
    "        for epoch_idx, (train_loss_val, val_loss_val) in enumerate(zip(training_losses, validation_losses), start=1):\n",
    "            writer.writerow([epoch_idx, train_loss_val, val_loss_val])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T13:39:52.375740Z",
     "start_time": "2024-09-10T13:39:52.373538Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def save_epoch_images(LR_img, HR_img, SR_img, HR_dim, epoch):\n",
    "    \"\"\"\n",
    "    Save the input, output, and high-resolution images for a given epoch.\n",
    "\n",
    "    Args:\n",
    "        LR_img (np.ndarray): Low-resolution input image.\n",
    "        HR_img (np.ndarray): High-resolution target image.\n",
    "        SR_img (np.ndarray): Super-resolved output image.\n",
    "        HR_dim (tuple): Dimensions for resizing the low-resolution image.\n",
    "        epoch (int): Current epoch number.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, dpi=150, figsize=(16, 5))\n",
    "\n",
    "    ax1.imshow(LR_img)\n",
    "    ax1.set_title('Low-Resolution Input')\n",
    "    ax2.imshow(HR_img)\n",
    "    ax2.set_title('High-Resolution Target')\n",
    "    ax3.imshow(SR_img)\n",
    "    ax3.set_title('Super-Resolution Output')\n",
    "    ax4.imshow(cv2.resize(LR_img, HR_dim, interpolation=cv2.INTER_CUBIC))\n",
    "    ax4.set_title('Bicubic Interpolation')\n",
    "\n",
    "    # for ax in (ax1, ax2, ax3, ax4):\n",
    "        # ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'trained-model/images/epoch_{epoch}.png')\n",
    "    # plt.show()\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T13:39:52.382499Z",
     "start_time": "2024-09-10T13:39:52.380823Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def prune_catalog(data_path):\n",
    "    \"\"\"\n",
    "    Delete the contents of the directory.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the dataset directory.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            os.remove(os.path.join(root, file))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T13:39:52.402533Z",
     "start_time": "2024-09-10T13:39:52.384307Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "\n",
    "class L1Loss(nn.Module):\n",
    "    \"\"\"L1 Loss (Mean Absolute Error)\"\"\"\n",
    "    def __init__(self):\n",
    "        super(L1Loss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        return F.l1_loss(prediction, target)\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"Perceptual Loss using specific layers from VGG-19\"\"\"\n",
    "    def __init__(self, layers=[1, 3, 5, 9, 13], lambda_weights=None, device='cpu'):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.lambda_weights = lambda_weights if lambda_weights is not None else [1.0] * len(layers)\n",
    "        self.device = device\n",
    "        \n",
    "        # Load pre-trained VGG-19\n",
    "        vgg = vgg19(pretrained=True).features.to(device).eval()\n",
    "        self.feature_extractors = [nn.Sequential(*list(vgg[:layer + 1])).to(device) for layer in layers]\n",
    "\n",
    "        for extractor in self.feature_extractors:\n",
    "            for param in extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        loss = 0.0\n",
    "        for i, extractor in enumerate(self.feature_extractors):\n",
    "            # Extract features\n",
    "            pred_features = extractor(prediction)\n",
    "            target_features = extractor(target)\n",
    "            \n",
    "            # Compute L1 loss on the features\n",
    "            d_features = F.l1_loss(pred_features, target_features, reduction='mean')\n",
    "            \n",
    "            # Apply lambda weights and accumulate the loss\n",
    "            loss += self.lambda_weights[i] * d_features\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class CombinedL1PerceptualLoss(nn.Module):\n",
    "    \"\"\"Combined L1 and Perceptual Loss\"\"\"\n",
    "    def __init__(self, alpha=1.0, beta=1.0, layers=[1, 3, 5, 9, 13], lambda_weights=None, device='cpu'):\n",
    "        super(CombinedL1PerceptualLoss, self).__init__()\n",
    "        self.l1_loss = L1Loss()\n",
    "        self.perceptual_loss = PerceptualLoss(layers=layers, lambda_weights=lambda_weights, device=device)\n",
    "        self.alpha = alpha  # Weight for L1 loss\n",
    "        self.beta = beta    # Weight for perceptual loss\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        # Move inputs to the same device as the loss functions\n",
    "        prediction = prediction.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        l1 = self.l1_loss(prediction, target)\n",
    "        perceptual = self.perceptual_loss(prediction, target)\n",
    "        total_loss = self.alpha * l1 + self.beta * perceptual\n",
    "        return total_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T13:39:52.407811Z",
     "start_time": "2024-09-10T13:39:52.399477Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.2694\n",
      "Epoch 1, Validation Loss: 0.1413\n",
      "Saved new best model at epoch 1\n",
      "Epoch 2, Training Loss: 0.1039\n",
      "Epoch 2, Validation Loss: 0.0945\n",
      "Saved new best model at epoch 2\n",
      "Epoch 3, Training Loss: 0.0783\n",
      "Epoch 3, Validation Loss: 0.0814\n",
      "Saved new best model at epoch 3\n",
      "Epoch 4, Training Loss: 0.0702\n",
      "Epoch 4, Validation Loss: 0.0742\n",
      "Saved new best model at epoch 4\n",
      "Epoch 5, Training Loss: 0.0643\n",
      "Epoch 5, Validation Loss: 0.0681\n",
      "Saved new best model at epoch 5\n",
      "Epoch 6, Training Loss: 0.0584\n",
      "Epoch 6, Validation Loss: 0.0620\n",
      "Saved new best model at epoch 6\n",
      "Epoch 7, Training Loss: 0.0528\n",
      "Epoch 7, Validation Loss: 0.0577\n",
      "Saved new best model at epoch 7\n",
      "Epoch 8, Training Loss: 0.0487\n",
      "Epoch 8, Validation Loss: 0.0546\n",
      "Saved new best model at epoch 8\n",
      "Epoch 9, Training Loss: 0.0464\n",
      "Epoch 9, Validation Loss: 0.0521\n",
      "Saved new best model at epoch 9\n",
      "Epoch 10, Training Loss: 0.0445\n",
      "Epoch 10, Validation Loss: 0.0509\n",
      "Saved new best model at epoch 10\n",
      "Epoch 11, Training Loss: 0.0426\n",
      "Epoch 11, Validation Loss: 0.0488\n",
      "Saved new best model at epoch 11\n",
      "Epoch 12, Training Loss: 0.0408\n",
      "Epoch 12, Validation Loss: 0.0472\n",
      "Saved new best model at epoch 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 62\u001B[0m\n\u001B[1;32m     59\u001B[0m SR_img \u001B[38;5;241m=\u001B[39m model(LR_crop)\n\u001B[1;32m     61\u001B[0m loss \u001B[38;5;241m=\u001B[39m l1_loss(SR_img, HR_crop)\n\u001B[0;32m---> 62\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     64\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/projects/super-res-in-face-images/venv/lib/python3.9/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/super-res-in-face-images/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup\n",
    "device = choose_device()\n",
    "model = RLFN_S(in_channels=3, out_channels=3).to(device)\n",
    "# model.load_state_dict(torch.load( \"epoch_2500.pth\"))\n",
    "\n",
    "# Initial optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(opt, step_size=200, gamma=0.5)\n",
    "\n",
    "# Load dataset and transforms\n",
    "train_data_path = \"train\"\n",
    "val_data_path = \"valid\"\n",
    "\n",
    "LR_dim = (64, 64)\n",
    "HR_dim = (256, 256)\n",
    "resize_obj = transforms.Resize(LR_dim, interpolation=InterpolationMode.BICUBIC)\n",
    "batch_size = 32\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(HR_dim, pad_if_needed=True),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(HR_dim[0], interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(HR_dim[0]),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = tv.datasets.ImageFolder(train_data_path, transform=transform)\n",
    "val_dataset = tv.datasets.ImageFolder(val_data_path, transform=val_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# Initialize losses\n",
    "l1_loss = L1Loss()\n",
    "l2_loss = nn.MSELoss()\n",
    "combined_loss = CombinedL1PerceptualLoss(alpha=1.0 / 256, beta=255.0 / 256, device=device)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_losses= []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop with warm-start policy\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        HR_crop = images\n",
    "        LR_crop = resize_obj(HR_crop).to(device)\n",
    "\n",
    "        SR_img = model(LR_crop)\n",
    "\n",
    "        loss = l1_loss(SR_img, HR_crop)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss);\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "\n",
    "    SR_img = []\n",
    "    HR_crop = []\n",
    "    LR_crop = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            HR_crop = images\n",
    "            LR_crop = resize_obj(HR_crop).to(device)\n",
    "\n",
    "            SR_img = model(LR_crop)\n",
    "            val_loss += l1_loss(SR_img, HR_crop).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'trained-model/saves/best_model_l1_loss.pth')\n",
    "        print(f'Saved new best model at epoch {epoch+1}')\n",
    "\n",
    "    # Save model periodically\n",
    "    if (epoch + 1) % 10 == 1:\n",
    "        torch.save(model.state_dict(), f'trained-model/saves/epoch_{epoch + 1}.pth')\n",
    "        idx = random.randint(0, SR_img.shape[0] - 1)\n",
    "        SR_img_single = tensor2uint(SR_img[idx])\n",
    "        HR_img = tensor2uint(HR_crop[idx])\n",
    "        LR_img = tensor2uint(LR_crop[idx])\n",
    "        save_epoch_images(LR_img, HR_img, SR_img_single, HR_dim, epoch + 1)\n",
    "        save_losses_to_csv(train_losses, val_losses, 'trained-model/saves/losses.csv')\n",
    "\n",
    "\n",
    "save_losses_to_csv(train_losses, val_losses, 'trained-model/saves/losses.csv')\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(opt, step_size=200, gamma=0.5)\n",
    "\n",
    "# Warm-start policy - continue training with the same settings\n",
    "for epoch in range(1000, 2000):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        HR_crop = images\n",
    "        LR_crop = resize_obj(HR_crop).to(device)\n",
    "\n",
    "        SR_img = model(LR_crop)\n",
    "\n",
    "        loss = l1_loss(SR_img, HR_crop)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss);\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    SR_img = []\n",
    "    HR_crop = []\n",
    "    LR_crop = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            HR_crop = images\n",
    "            LR_crop = resize_obj(HR_crop).to(device)\n",
    "\n",
    "            SR_img = model(LR_crop)\n",
    "            val_loss += l1_loss(SR_img, HR_crop).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'trained-model/saves/best_model_l1_loss.pth')\n",
    "        print(f'Saved new best model at epoch {epoch+1}')\n",
    "\n",
    "    # Save model periodically\n",
    "    if (epoch + 1) % 10 == 1:\n",
    "        torch.save(model.state_dict(), f'trained-model/saves/epoch_{epoch + 1}.pth')\n",
    "        idx = random.randint(0, SR_img.shape[0] - 1)\n",
    "        SR_img_single = tensor2uint(SR_img[idx])\n",
    "        HR_img = tensor2uint(HR_crop[idx])\n",
    "        LR_img = tensor2uint(LR_crop[idx])\n",
    "        save_epoch_images(LR_img, HR_img, SR_img_single, HR_dim, epoch + 1)\n",
    "        save_losses_to_csv(train_losses, val_losses, 'trained-model/saves/losses.csv')\n",
    "\n",
    "save_losses_to_csv(train_losses, val_losses, 'trained-model/saves/losses.csv')\n",
    "\n",
    "\n",
    "# After initial training and warm-start, switch to L1 + Perceptual loss and retrain with warm-start policy\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(opt, step_size=200, gamma=0.5)\n",
    "best_val_loss = best_val_loss = float('inf')\n",
    "\n",
    "# Fine-tune model\n",
    "for epoch in range(2000, 2300):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        HR_crop = images\n",
    "        LR_crop = resize_obj(HR_crop).to(device)\n",
    "\n",
    "        SR_img = model(LR_crop)\n",
    "\n",
    "        loss = combined_loss(SR_img, HR_crop)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss);\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    SR_img = []\n",
    "    HR_crop = []\n",
    "    LR_crop = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            HR_crop = images\n",
    "            LR_crop = resize_obj(HR_crop).to(device)\n",
    "\n",
    "            SR_img = model(LR_crop)\n",
    "            val_loss += combined_loss(SR_img, HR_crop).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'trained-model/saves/best_model_combined.pth')\n",
    "        print(f'Saved new best model at epoch {epoch+1}')\n",
    "\n",
    "    # Save model periodically\n",
    "    if (epoch + 1) % 10 == 1:\n",
    "        torch.save(model.state_dict(), f'trained-model/saves/epoch_{epoch + 1}.pth')\n",
    "        idx = random.randint(0, SR_img.shape[0] - 1)\n",
    "        SR_img_single = tensor2uint(SR_img[idx])\n",
    "        HR_img = tensor2uint(HR_crop[idx])\n",
    "        LR_img = tensor2uint(LR_crop[idx])\n",
    "        save_epoch_images(LR_img, HR_img, SR_img_single, HR_dim, epoch + 1)\n",
    "        save_losses_to_csv(train_losses, val_losses, 'trained-model/saves/losses.csv')\n",
    "\n",
    "\n",
    "save_losses_to_csv(train_losses, val_losses, 'trained-model/saves/losses.csv')\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(opt, step_size=200, gamma=0.5)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(2300, 2501):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        HR_crop = images\n",
    "        LR_crop = resize_obj(HR_crop).to(device)\n",
    "\n",
    "        SR_img = model(LR_crop)\n",
    "\n",
    "        loss = l2_loss(SR_img, HR_crop)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss);\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    SR_img = []\n",
    "    HR_crop = []\n",
    "    LR_crop = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            HR_crop = images\n",
    "            LR_crop = resize_obj(HR_crop).to(device)\n",
    "\n",
    "            SR_img = model(LR_crop)\n",
    "            val_loss += l2_loss(SR_img, HR_crop).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'trained-model/saves/best_model_l2_loss.pth')\n",
    "        print(f'Saved new best model at epoch {epoch+1}')\n",
    "\n",
    "    # Save model periodically\n",
    "    if (epoch + 1) % 10 == 1:\n",
    "        torch.save(model.state_dict(), f'trained-model/saves/epoch_{epoch + 1}.pth')\n",
    "        idx = random.randint(0, SR_img.shape[0] - 1)\n",
    "        SR_img_single = tensor2uint(SR_img[idx])\n",
    "        HR_img = tensor2uint(HR_crop[idx])\n",
    "        LR_img = tensor2uint(LR_crop[idx])\n",
    "        save_epoch_images(LR_img, HR_img, SR_img_single, HR_dim, epoch + 1)\n",
    "        save_losses_to_csv(train_losses, val_losses, 'trained-model/saves/losses.csv')\n",
    "\n",
    "save_losses_to_csv(train_losses, val_losses, 'trained-model/saves/losses.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T13:44:13.929429Z",
     "start_time": "2024-09-10T13:39:52.401067Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T13:44:13.932752Z",
     "start_time": "2024-09-10T13:44:13.932304Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(image_path, device):\n",
    "    \"\"\"\n",
    "    Preprocess the input image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        LR_dim (tuple): Dimensions to resize the low-resolution image.\n",
    "        device (torch.device): Device to use (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.Resize(LR_dim),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image.to(device)\n",
    "\n",
    "def super_resolve_image(model, image_path, upscale_factor, device, output_image_path):\n",
    "    \"\"\"\n",
    "    Super-resolve an image using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained super-resolution model.\n",
    "        image_path (str): Path to the input low-resolution image.\n",
    "        LR_dim (tuple): Low-resolution dimensions expected by the model.\n",
    "        HR_dim (tuple): High-resolution dimensions for display.\n",
    "        device (torch.device): Device to use (CPU or GPU).\n",
    "        output_image_path (str): Path to save the super-resolved output image.\n",
    "    \"\"\"\n",
    "    # Preprocess the input image\n",
    "    input_image = preprocess_image(image_path, device)\n",
    "\n",
    "    # Run the model on the preprocessed image\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        SR_image = model(input_image)\n",
    "\n",
    "    # Postprocess the output image\n",
    "    output_image = tensor2uint(SR_image)\n",
    "\n",
    "    input2 = tensor2uint(input_image)\n",
    "\n",
    "    # Save the output image\n",
    "    Image.fromarray(output_image).save(output_image_path)\n",
    "\n",
    "    # Load the original image for comparison\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    original_image = np.array(original_image)\n",
    "    original_height, original_width = original_image.shape[:2]\n",
    "\n",
    "    new_width = int(original_width * upscale_factor)\n",
    "    new_height = int(original_height * upscale_factor)\n",
    "\n",
    "    bicubic_image = cv2.resize(original_image, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # Plot the input, output, and bicubic images\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title('Low-Resolution Input')\n",
    "\n",
    "    axes[1].imshow(output_image)\n",
    "    axes[1].set_title('Super-Resolved Output')\n",
    "\n",
    "    axes[2].imshow(bicubic_image)\n",
    "    axes[2].set_title('Bicubic Interpolation')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "device = choose_device()\n",
    "\n",
    "# --------------------------------\n",
    "# load model\n",
    "# --------------------------------\n",
    "\n",
    "model = RLFN_S(in_channels=3, out_channels=3)\n",
    "\n",
    "model.load_state_dict(torch.load( \"trained-model/saves/best_model_l2_loss.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "image_path = 'output_faces/face_39.jpg'  # Replace with the path to your low-res input image\n",
    "# LR_dim = (64, 64)  # Replace with your model's low-res input dimensions\n",
    "# HR_dim = (256, 256)  # Replace with your desired high-res output dimensions\n",
    "upscale_factor = 4\n",
    "output_image_path = 'super_resolved_image.png'  # Replace with the desired path to save the output image\n",
    "\n",
    "super_resolve_image(model, image_path, upscale_factor, device, output_image_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-10T13:44:13.932691Z"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4mK8IZYk3Q4b",
    "9ZeF_tIMlzyr",
    "heqZ-nSHakue",
    "y_au-qKGmkqY",
    "UoB8UX3wwguq",
    "hT6jqVWtzsOb",
    "GOHEYEt30HEM",
    "l8YiJA2zeFOv",
    "lXsMZyfGaqMv"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
